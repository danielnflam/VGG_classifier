{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7939c6bd-0a8d-4375-95db-3981e73d19fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Total Num Folds: 10\n",
      "Total Num Epochs: 25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, ConcatDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os, sys, math, random, copy, time\n",
    "import datasets, custom_transforms, models\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 0\n",
    "split_k_folds=10\n",
    "flag_selectFold, selectedFold = False, None\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Parameters to Change\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 4\n",
    "num_epochs_classifier=5\n",
    "num_epochs_finetune=20\n",
    "finetune_lr = 5e-6\n",
    "finetune_weightdecay = 0.18\n",
    "\n",
    "top_cutoff_percent = 0.15 # 0.08 COVID-Net paper by Wang et al.\n",
    "flag_includePneumoniaPics = False\n",
    "\n",
    "TRAINVAL_DATA_DIR = 'G:/DanielLam/DanielCustomNetwork/DanielDataSets_Reduced_256_None/trainVal/'\n",
    "save_folder_name =  os.path.join(\"15cutoff_fineTune_G4000R217\", \"VGG16_Rajaraman_None_NORMALCOVIDonly\")\n",
    "\n",
    "print(\"Total Num Folds: {}\".format(split_k_folds))\n",
    "print(\"Total Num Epochs: {}\".format(num_epochs_classifier + num_epochs_finetune))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0779c6-83f1-420a-aec5-6ce4d4050920",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c72d626-cde7-45e2-87fe-11391a51394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCKED\n",
    "# Preprocessing settings:\n",
    "interp_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "image_size_to_VGG =224 # resize input images to this size in VGG\n",
    "model_name = \"VGG\"\n",
    "num_channels = 3 # because VGG is trained on RGB images\n",
    "unbias=True\n",
    "limit_save_epoch_acc=0.80\n",
    "\n",
    "\n",
    "# Learning Rates\n",
    "betas = (0.9,0.999)\n",
    "ini_lr= 1e-3\n",
    "max_layers_to_freeze = 28 # 30 means that only the last linear classifier is left to train\n",
    "class_weights = torch.Tensor([1, 1100/996]) # NONCOVID, COVID\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1cfb2-cc4d-4dc2-b485-2e617e9934ee",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907a6aea-da68-46b1-9b6a-85eef2747b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Transforms\n",
    "# Assumes importing images as PIL Images\n",
    "normalize = torchvision.transforms.Normalize(\n",
    "                                [0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])\n",
    "\n",
    "class Equalise(object):\n",
    "    # Histogram equalisation\n",
    "    # Only works on PIL images\n",
    "    def __init__(self, mask=None):\n",
    "        self.mask = mask\n",
    "    def __call__(self, image):\n",
    "        return ImageOps.equalize(image, self.mask)\n",
    "    \n",
    "class ImageComplement(object):\n",
    "    # Flip image intensities (i.e. black becomes white, white becomes black)\n",
    "    # Assumes input image is a tensor with range[0,1]\n",
    "    def __init__(self, probability=0.5):\n",
    "        self.probability = probability\n",
    "    def __call__(self, image):\n",
    "        activate = np.random.uniform(0,1)\n",
    "        if activate < self.probability:\n",
    "            # Flip\n",
    "            max_image = torch.max(image)\n",
    "            min_image = torch.min(image)\n",
    "            #print(\"min:{},max:{}\".format(min_image, max_image))\n",
    "            image = (image-min_image)/(max_image-min_image) # range [0,1]\n",
    "            image = (1-image)*(max_image-min_image) + min_image # back to original range\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "train_transforms=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    custom_transforms.COVIDNetProcessing(top_cutoff_percent),\n",
    "                                    torchvision.transforms.Resize(image_size_to_VGG, interpolation=interp_mode), # PIL image\n",
    "                                    torchvision.transforms.CenterCrop(image_size_to_VGG),\n",
    "                                    torchvision.transforms.RandomHorizontalFlip(), \n",
    "                                    torchvision.transforms.ColorJitter(brightness=0.1), # this jitters brightness\n",
    "                                    torchvision.transforms.RandomAffine(degrees=10,translate=(0.1,0.1),scale=(0.9,1.1)),\n",
    "                                    ImageComplement(),\n",
    "                                    normalize,\n",
    "                                    ])\n",
    "\n",
    "val_transforms=torchvision.transforms.Compose([\n",
    "                                    torchvision.transforms.ToTensor(),\n",
    "                                    custom_transforms.COVIDNetProcessing(top_cutoff_percent),\n",
    "                                    torchvision.transforms.Resize(image_size_to_VGG, interpolation=interp_mode),\n",
    "                                    torchvision.transforms.CenterCrop(image_size_to_VGG),\n",
    "                                    normalize,\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6a3ee-04ce-4476-8392-5df8ef981f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e10bdf-793e-4cf0-a5f3-af0aac5a9555",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0964cfc4-530b-46b3-bc01-35fa4cba5014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None Path\n",
      "RAW DATASET STATISTICS:-----------\n",
      "Normal:1100, Pneumonia:0, COVID:996\n",
      "2-Class Dataset Statistics:-----------\n",
      "Noncovid:1100, COVID:996\n",
      "===========\n",
      "Fold 1\n",
      "Breaking loop.\n",
      "Fold 2\n",
      "Breaking loop.\n",
      "Fold 3\n",
      "Breaking loop.\n",
      "Fold 4\n",
      "Breaking loop.\n",
      "Fold 5\n",
      "Breaking loop.\n",
      "Fold 6\n",
      "Breaking loop.\n",
      "Fold 7\n",
      "Breaking loop.\n",
      "Fold 8\n",
      "Breaking loop.\n",
      "Fold 9\n",
      "===========\n",
      "===========\n",
      "dict_keys(['name', 'image', 'label'])\n",
      "dict_keys(['name', 'image', 'label'])\n",
      "Parameters total:  14715714\n",
      "Parameters trainable:  1026\n",
      "Epoch 1/5\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.4539 Acc: 0.8156\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.2614 Acc: 0.9234\n",
      "Epoch 2/5\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.3207 Acc: 0.8866\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.2000 Acc: 0.9426\n",
      "Epoch 3/5\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.2994 Acc: 0.8887\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.1744 Acc: 0.9522\n",
      "Epoch 4/5\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.2812 Acc: 0.8850\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.1638 Acc: 0.9474\n",
      "Epoch 5/5\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.2511 Acc: 0.9136\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.1496 Acc: 0.9569\n",
      "Training complete in 2m 33s\n",
      "Best val Acc: 0.000000\n",
      "+++++++\n",
      "Fine-Tuning\n",
      "+++++++\n",
      "Parameters total:  14715714\n",
      "Parameters trainable:  7080450\n",
      "Epoch 6/25\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.2526 Acc: 0.9073\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.1485 Acc: 0.9522\n",
      "Epoch 7/25\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.2470 Acc: 0.9120\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.1474 Acc: 0.9522\n",
      "Epoch 8/25\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.2477 Acc: 0.9120\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.1464 Acc: 0.9522\n",
      "Epoch 9/25\n",
      "----------\n",
      "Length of train dataset: 1887\n",
      "train Loss: 0.2497 Acc: 0.9025\n",
      "Length of val dataset: 209\n",
      "val Loss: 0.1445 Acc: 0.9522\n",
      "Epoch 10/25\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-436733c05cf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                  \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                                  \u001b[0mcheckpoint_save_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m                                  is_inception=(model_name==\"inception\"))\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[0mfoldperf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fold{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-436733c05cf0>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, num_epochs, history, checkpoint_save_path, scheduler, is_inception)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m# statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, history=None, checkpoint_save_path = None,\n",
    "                scheduler=None, is_inception=False):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    try:\n",
    "        history_accuracy = {\"train\":history[\"accuracy\"][\"train\"], \"val\":history[\"accuracy\"][\"val\"]}\n",
    "        history_loss = {\"train\":history[\"loss\"][\"train\"], \"val\":history[\"loss\"][\"val\"]}\n",
    "    except:\n",
    "        history_accuracy = {\"train\":[], \"val\":[]}\n",
    "        history_loss = {\"train\":[], \"val\":[]}\n",
    "    \n",
    "    best_loss = 1.0\n",
    "    best_acc = 0.0\n",
    "    # Is this continuing from former training?\n",
    "    pretrained_epochs = len(history_accuracy[\"train\"])\n",
    "    \n",
    "    for epoch in range(pretrained_epochs+1, pretrained_epochs+num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, pretrained_epochs+num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in dataloaders.keys():\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                inputs=data[\"image\"]\n",
    "                labels=data[\"label\"]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                labels = labels.detach().cpu()\n",
    "                preds = preds.detach().cpu()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # for each element in preds, check is equal to each element in labels.data\n",
    "                running_corrects += torch.sum(torch.eq(preds, labels))\n",
    "                #print(running_corrects)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].sampler)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].sampler)\n",
    "            print(\"Length of {} dataset: {}\".format(phase, len(dataloaders[phase].sampler)))\n",
    "            \n",
    "            # Record histories\n",
    "            history_accuracy[phase].append(epoch_acc)\n",
    "            history_loss[phase].append(epoch_loss)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_outputs = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': best_model_wts,\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'N_CLASSES': N_CLASSES,\n",
    "                    'BATCH_SIZE': BATCH_SIZE,\n",
    "                    'ini_lr': ini_lr,\n",
    "                    'max_layers_to_freeze':max_layers_to_freeze,\n",
    "                    'class_weights':class_weights,\n",
    "                    'random_seed': seed\n",
    "            }\n",
    "            if phase == 'val' and epoch_acc > limit_save_epoch_acc and checkpoint_save_path is not None:\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                print(\"Achieved current lowest loss of: {} ; accuracy = {}\".format(best_loss, epoch_acc))\n",
    "                saved_checkpoint = os.path.join(checkpoint_save_path,\n",
    "                             'balance[epoch_{}_loss_{:.3f}_acc_{:.3f}].pt'.format((epoch), history_loss[phase][-1],\n",
    "                                                                                        history_accuracy[phase][-1]))\n",
    "                torch.save(save_outputs, saved_checkpoint)\n",
    "                print(\"Saved checkpoint: {}\".format(saved_checkpoint))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    history = {\"accuracy\":history_accuracy, \"loss\":history_loss}\n",
    "    return model, history, save_outputs\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    # If feature extracting only.\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "import datasets\n",
    "import models\n",
    "splits=StratifiedKFold(n_splits=split_k_folds,shuffle=True,random_state=42)\n",
    "foldperf={}\n",
    "def pneumoniaPath(TRAINVAL_DATA_DIR,  flag_includePneumoniaPics):\n",
    "    if flag_includePneumoniaPics:\n",
    "        return os.path.join(TRAINVAL_DATA_DIR,\"PNEUMONIA\")\n",
    "    else:\n",
    "        return None\n",
    "trainVal_dataset = datasets.Coviddataset(normal_path=os.path.join(TRAINVAL_DATA_DIR,\"NORMAL\"),\n",
    "                                      pneumonia_path= pneumoniaPath(TRAINVAL_DATA_DIR, flag_includePneumoniaPics),\n",
    "                                      covid_path=os.path.join(TRAINVAL_DATA_DIR,\"COVID\"),\n",
    "                                      transform = None,\n",
    "                                     NClasses=N_CLASSES, unbias=unbias,channels=num_channels, display_console=True)\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(X=np.arange(len(trainVal_dataset)), y=trainVal_dataset.labels)):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    \n",
    "    # Select a fold?\n",
    "    if flag_selectFold:\n",
    "        if selectedFold is not None and selectedFold >= 1 and selectedFold-1 <= split_k_folds:\n",
    "            if fold != selectedFold-1:\n",
    "                print(\"Breaking loop.\")\n",
    "                continue\n",
    "        else:\n",
    "            raise RuntimeError(\"selectedFold should be a number between 1 and split_k_folds\")\n",
    "            \n",
    "    \n",
    "    # Subset sample from dataset\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    dl_training = DataLoader(datasets.Coviddataset(normal_path=os.path.join(TRAINVAL_DATA_DIR,\"NORMAL\"),\n",
    "                                      pneumonia_path=pneumoniaPath(TRAINVAL_DATA_DIR, flag_includePneumoniaPics),\n",
    "                                      covid_path=os.path.join(TRAINVAL_DATA_DIR,\"COVID\"),\n",
    "                                      transform = train_transforms,\n",
    "                                     NClasses=N_CLASSES, unbias=unbias,channels=num_channels, display_console=False),\n",
    "                             batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=0)\n",
    "    dl_validation = DataLoader(datasets.Coviddataset(normal_path=os.path.join(TRAINVAL_DATA_DIR,\"NORMAL\"),\n",
    "                                      pneumonia_path=pneumoniaPath(TRAINVAL_DATA_DIR, flag_includePneumoniaPics),\n",
    "                                      covid_path=os.path.join(TRAINVAL_DATA_DIR,\"COVID\"),\n",
    "                                      transform = val_transforms,\n",
    "                                     NClasses=N_CLASSES, unbias=unbias,channels=num_channels, display_console=False),\n",
    "                             batch_size=BATCH_SIZE, sampler=test_sampler, num_workers=0)\n",
    "    \n",
    "    dataloaders = {'train':dl_training,'val':dl_validation}\n",
    "    \n",
    "    # DEBUGGING\n",
    "    for phase in dataloaders.keys():\n",
    "        sample = next(iter(dataloaders[phase]))\n",
    "        print(sample.keys())\n",
    "        out = torchvision.utils.make_grid(sample[\"image\"])\n",
    "        #imshow(out)\n",
    "    \n",
    "    # First training using Adam\n",
    "    # Model\n",
    "    model = models.VGG16_Rajaraman(N_CLASSES, max_layer_to_freeze=30, verbose=False) # freeze all layers\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=ini_lr, betas=betas)\n",
    "\n",
    "    # Deterministic\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Train model & save outputs\n",
    "    model, history, save_outputs = train_model(model, dataloaders, criterion, optimizer, num_epochs_classifier,\n",
    "                                 history=None,\n",
    "                                 checkpoint_save_path=None,\n",
    "                                 is_inception=(model_name==\"inception\"))\n",
    "    print(\"+++++++\")\n",
    "    print(\"Fine-Tuning\")\n",
    "    print(\"+++++++\")\n",
    "    # UnFreeze layers\n",
    "    model.freeze_layers_VGG16(layers=np.arange(24,30), freeze=False) # unfreeze convblock 5\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('Parameters total: ',total_num)\n",
    "    print('Parameters trainable: ',trainable_num)\n",
    "    # Train using slower method\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=finetune_lr, weight_decay=finetune_weightdecay)\n",
    "    model, history, save_outputs = train_model(model, dataloaders, criterion, optimizer, num_epochs_finetune,\n",
    "                                 history=history,\n",
    "                                 checkpoint_save_path=None,\n",
    "                                 is_inception=(model_name==\"inception\"))\n",
    "    foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "    # Saving Output\n",
    "    checkpoint_save_path=os.path.join('checkpoints', str(N_CLASSES)+\"Class\", save_folder_name)\n",
    "    a = Path(checkpoint_save_path)\n",
    "    a.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    save_filename = \"Fold{}[classifier{}_finetune{}].pt\".format(fold+1, num_epochs_classifier, num_epochs_finetune)\n",
    "    saved_checkpoint = os.path.join(checkpoint_save_path, save_filename)\n",
    "    torch.save(save_outputs, saved_checkpoint)\n",
    "    print(\"Saved checkpoint: {}\".format(saved_checkpoint))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbdccbc-08ff-462f-94a9-ad2c9e834fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete\")\n",
    "# Find the best fold\n",
    "flag_debug=False\n",
    "output_save_directory=checkpoint_save_path\n",
    "\n",
    "# AVERAGE PERFORMANCE\n",
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=len(foldperf.keys())#split_k_folds\n",
    "if selectedFold is None:\n",
    "    initial_fold = 1\n",
    "else:\n",
    "    initial_fold = selectedFold\n",
    "fold_range = range(initial_fold,initial_fold+k)\n",
    "for f in fold_range:\n",
    "    tl_f.append(np.mean(foldperf['fold{}'.format(f)]['loss']['train']))\n",
    "    testl_f.append(np.mean(foldperf['fold{}'.format(f)]['loss']['val']))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf['fold{}'.format(f)]['accuracy']['train']))\n",
    "    testa_f.append(np.mean(foldperf['fold{}'.format(f)]['accuracy']['val']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.3f} \\t Average Test Acc: {:.3f}\"\n",
    "      .format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f)))\n",
    "\n",
    "if initial_fold == 1:\n",
    "    print(\"Best fold: {}\".format(np.argmax(testa_f)+1))\n",
    "\n",
    "import json\n",
    "save_dict = {\"Loss\":{\"Training\":np.mean(tl_f),\"Validation\":np.mean(testl_f)}, \n",
    "             \"Accuracy\":{\"Training\":np.mean(ta_f), \"Validation\":np.mean(testa_f)}}\n",
    "with open(os.path.join(output_save_directory,'data.json'), 'w') as fp:\n",
    "    json.dump(save_dict, fp)\n",
    "\n",
    "# Averaging accuracy and loss\n",
    "diz_ep = {'train_loss_ep':[],'test_loss_ep':[],'train_acc_ep':[],'test_acc_ep':[]}\n",
    "for i in range(0, num_epochs_classifier+num_epochs_finetune):\n",
    "    diz_ep['train_loss_ep'].append(np.mean([foldperf['fold{}'.format(f)]['loss']['train'][i] for f in fold_range]))\n",
    "    diz_ep['test_loss_ep'].append(np.mean([foldperf['fold{}'.format(f)]['loss']['val'][i] for f in fold_range]))\n",
    "    diz_ep['train_acc_ep'].append(np.mean([foldperf['fold{}'.format(f)]['accuracy']['train'][i] for f in fold_range]))\n",
    "    diz_ep['test_acc_ep'].append(np.mean([foldperf['fold{}'.format(f)]['accuracy']['val'][i] for f in fold_range]))\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_loss_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_loss_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, \"average_fold_loss\"+\".png\"))\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_acc_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_acc_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, \"average_fold_accuracy\"+\".png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34393c15-00d5-46fd-8789-f4cdf0e400fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f)\n",
    "print(len(foldperf['fold1']['loss']['val']))\n",
    "\n",
    "print(\"Length of validation idx: {}\".format(len(val_idx)))\n",
    "len(dl_validation.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52cb27-1daa-4fd8-b2dd-eb0a2d05bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_list= list(i for i in range(1,num_epochs+1))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,8))\n",
    "ax[0].set(title='Train/Val Loss', xlabel='Epoch', ylabel='Loss' )\n",
    "ax[0].plot(epoch_list, history[\"loss\"]['train'],linewidth=2,linestyle=':',label='Train Loss', marker='o')\n",
    "ax[0].plot(epoch_list, history[\"loss\"]['val'], linewidth=1, linestyle='--', label='Val Loss', marker='+')\n",
    "ax[0].legend(loc=2)\n",
    "\n",
    "ax[1].set(title='Train/Val Acc', xlabel='Epoch', ylabel='Acc' )\n",
    "ax[1].plot(epoch_list, history[\"accuracy\"]['train'], linewidth=2, linestyle=':', label='Train Acc', marker='o')\n",
    "ax[1].plot(epoch_list, history[\"accuracy\"]['val'], linewidth=1, linestyle='--', label='Val Acc', marker='+')\n",
    "ax[1].legend(loc=2)\n",
    "plt.savefig(os.path.join(checkpoint_save_path,\"lossaccpic_\"+str(N_CLASSES)+\"class.png\"), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
