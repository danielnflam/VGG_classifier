{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad2c1ef-9d36-48b2-bca3-7ab67e476960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os, sys, math, random, copy, time\n",
    "import datasets, custom_transforms, models\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ae367-f838-4d29-8383-9cc576fcbbf8",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e75e97-d8cf-4122-be49-f89f39dac00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\\2Class\\VGG16_Rajaraman_Rajaraman\\28\\balance[epoch_12_loss_0.066_acc_0.990].pt\n"
     ]
    }
   ],
   "source": [
    "# CKPT for vlassifier\n",
    "#description = 'DanielCustomNetwork_Original_VGG16Rajaraman'\n",
    "#CKPT_filename = 'balance[epoch_10_loss_0.059_acc_0.990]'+'.pt'\n",
    "\n",
    "training_data = \"Rajaraman\"\n",
    "\n",
    "description = 'VGG16_Rajaraman_'+training_data\n",
    "if training_data is \"None\":\n",
    "    CKPT_filename = 'balance[epoch_12_loss_0.090_acc_0.990]'+'.pt' #'balance[epoch_4_loss_0.060_acc_0.980]'+'.pt'\n",
    "elif training_data is \"Gusarev\":\n",
    "    CKPT_filename = 'balance[epoch_12_loss_0.058_acc_0.990]'+'.pt'\n",
    "elif training_data is \"Rajaraman\":\n",
    "    CKPT_filename = 'balance[epoch_12_loss_0.066_acc_0.990]'+'.pt'#'balance[epoch_6_loss_0.055_acc_0.985]'+'.pt'\n",
    "\n",
    "max_layers_to_freeze = 28\n",
    "\n",
    "\n",
    "# SETTINGS\n",
    "N_CLASSES = 2\n",
    "unbias_dataset = False # False to use ALL data in the dataset, True to cut the number of elements in each set to roughly the same number.\n",
    "CLASS_NAMES = ['NONCOVID', 'COVID' ]#['Normal', 'Pneumonia', 'COVID-19'] #  in order of 0, 1, 2\n",
    "BATCH_SIZE = 16\n",
    "switch_pair = [\"internal_test_Daniel\", \"NoSuppression\", \"Gusarev\", \"Rajaraman\"]\n",
    "interp_mode = torchvision.transforms.InterpolationMode.NEAREST\n",
    "top_cutoff_percent = 0.08 # 0.08 is same as in COVID-Net\n",
    "image_size_to_VGG = 224 # VGG input size\n",
    "\n",
    "\n",
    "CKPT_PATH = os.path.join(description , str(max_layers_to_freeze), CKPT_filename)\n",
    "CKPT_PATH = os.path.join('checkpoints', str(N_CLASSES)+\"Class\", CKPT_PATH)\n",
    "print(CKPT_PATH)\n",
    "# Dataset saving directories\n",
    "save_picture_directory = Path(os.path.join(\"reports\", str(N_CLASSES)+\"Class\", str(description))) # directory to save confusion matrices\n",
    "save_picture_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Transforms\n",
    "normalize = torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225]) # ImageNet mean and std for normalisation\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            custom_transforms.COVIDNetProcessing(top_cutoff_percent),\n",
    "                            torchvision.transforms.Resize(image_size_to_VGG, interpolation=interp_mode),\n",
    "                            torchvision.transforms.CenterCrop(image_size_to_VGG),\n",
    "                            normalize,\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8b827-031b-474c-87c9-d64293bf7443",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a00945b-d46b-49a6-8bf6-d75903196f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normal={\n",
    "    \"NoSuppression\": \"G:/DanielLam/DanielBoneSuppressedData/NoSuppression/PYNEH_onePerPatient_NORMALS_noSuppression__uncropped/\",\n",
    "    \"Gusarev\": \"G:/DanielLam/DanielBoneSuppressedData/Gusarev/PYNEH_onePerPatient_NORMALS_10KFold217_uncropped/\",\n",
    "    \"Rajaraman\": \"G:/DanielLam/DanielBoneSuppressedData/Rajaraman/PYNEH_onePerPatient_NORMALS_suppressed_10KFold217_uncropped/\",\n",
    "               }\n",
    "dataset_pneumonia={\n",
    "    \"NoSuppression\":None,\n",
    "    \"Gusarev\":None,\n",
    "    \"Rajaraman\":None,\n",
    "                  }\n",
    "dataset_covid={\n",
    "    \"NoSuppression\":\"G:/DanielLam/DanielBoneSuppressedData/NoSuppression/QEH_onePerPatient_all_noSuppression_10KFold__uncropped/\",\n",
    "    \"Gusarev\":\"G:/DanielLam/DanielBoneSuppressedData/Gusarev/QEH_onePerPatient_all_10KFold4000_uncropped/\",\n",
    "    \"Rajaraman\":\"G:/DanielLam/DanielBoneSuppressedData/Rajaraman/QEH_onePerPatient_all_suppressed_10KFold4000_uncropped/\",\n",
    "              }\n",
    "\n",
    "def switchDataset(switch, dataset_normal=None, dataset_pneumonia=None, dataset_covid=None, training_data=None):\n",
    "    if switch == \"internal_test_Daniel\":\n",
    "        data_normal = \"G:/DanielLam/DanielCustomNetwork/DanielDataSets_Reduced_256_\"+str(training_data_text)+\"/test/NORMAL/\"\n",
    "        data_pneumonia = \"G:/DanielLam/DanielCustomNetwork/DanielDataSets_Reduced_256_\"+str(training_data_text)+\"/test/PNEUMONIA/\"\n",
    "        data_covid = \"G:/DanielLam/DanielCustomNetwork/DanielDataSets_Reduced_256_\"+str(training_data_text)+\"/test/COVID/\"\n",
    "    else:\n",
    "        data_normal = dataset_normal[switch]\n",
    "        data_pneumonia = dataset_pneumonia[switch]\n",
    "        data_covid = dataset_covid[switch]\n",
    "    return {'normal': data_normal, 'pneumonia': data_pneumonia, 'covid': data_covid}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0455c4-c37b-4f4a-a886-f1ac86d962a3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98acd8ad-29c6-4fb8-b26a-979176d5f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters total:  14715714\n",
      "Parameters trainable:  2360834\n",
      "=> loading checkpoint\n",
      "=> loaded checkpoint\n",
      "Model Loaded.\n"
     ]
    }
   ],
   "source": [
    "model = models.VGG16_Rajaraman(N_CLASSES, max_layer_to_freeze=max_layers_to_freeze, verbose=False)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "if os.path.isfile(CKPT_PATH):\n",
    "    print(\"=> loading checkpoint\")\n",
    "    checkpoint = torch.load(CKPT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    raise RuntimeError(\"=> no checkpoint found\")\n",
    "    \n",
    "print(\"Model Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f164d5c0-3141-43b1-ae15-38dca9e6d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:0.001 , Batch Size:4, Class Weights:tensor([1.0000, 1.1161], device='cuda:0'), FrozenLayers:28, Num Classes:2\n",
      "[{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"LR:{} , Batch Size:{}, Class Weights:{}, FrozenLayers:{}, Num Classes:{}\".format(checkpoint[\"ini_lr\"],\n",
    "                        checkpoint[\"BATCH_SIZE\"], checkpoint['class_weights'], checkpoint[\"max_layers_to_freeze\"], checkpoint[\"N_CLASSES\"]))\n",
    "except:\n",
    "    print(\"Missing one of the dict keys\")\n",
    "print(checkpoint[\"optimizer_state_dict\"][\"param_groups\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e8fb5-ee82-4aa0-8749-08edb2bdd3f6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d60c03-44e7-4e30-a5b0-217883d14de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATASET STATISTICS:-----------\n",
      "Normal:100, Pneumonia:100, COVID:100\n",
      "2-Class Dataset Statistics:-----------\n",
      "Noncovid:200, COVID:100\n",
      "===========\n",
      "None Path\n",
      "RAW DATASET STATISTICS:-----------\n",
      "Normal:564, Pneumonia:0, COVID:524\n",
      "2-Class Dataset Statistics:-----------\n",
      "Noncovid:564, COVID:524\n",
      "===========\n",
      "None Path\n",
      "RAW DATASET STATISTICS:-----------\n",
      "Normal:564, Pneumonia:0, COVID:524\n",
      "2-Class Dataset Statistics:-----------\n",
      "Noncovid:564, COVID:524\n",
      "===========\n",
      "None Path\n",
      "RAW DATASET STATISTICS:-----------\n",
      "Normal:564, Pneumonia:0, COVID:524\n",
      "2-Class Dataset Statistics:-----------\n",
      "Noncovid:564, COVID:524\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "true_labels={}\n",
    "for switch in switch_pair:\n",
    "    # What is the training data used to train the network?\n",
    "    if training_data is not None:\n",
    "        if training_data is \"NoSuppression\":\n",
    "            training_data_text = \"None\"\n",
    "        else:\n",
    "            training_data_text = training_data\n",
    "    # Dataset\n",
    "    chosen_datasets = switchDataset(switch, dataset_normal, dataset_pneumonia, dataset_covid, training_data_text)\n",
    "    test_dataset = datasets.Coviddataset(normal_path=chosen_datasets[\"normal\"], \n",
    "                                         pneumonia_path=chosen_datasets[\"pneumonia\"], \n",
    "                                         covid_path=chosen_datasets[\"covid\"], \n",
    "                                         transform = test_transforms, NClasses=N_CLASSES, \n",
    "                                         unbias=unbias_dataset, channels=3) # RGB image\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Classify\n",
    "    predictions[switch] = []\n",
    "    true_labels[switch] = []\n",
    "    for data in test_loader:\n",
    "        inputs, labels, names = data['image'], data['label'], data['name']\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs)\n",
    "            preds = preds.detach().clone()\n",
    "            preds = torch.argmax(preds,1,False)\n",
    "        \n",
    "        # Append lists\n",
    "        predictions[switch].append(preds.detach().cpu())\n",
    "        true_labels[switch].append(labels.detach().cpu())\n",
    "    predictions[switch] = torch.cat(predictions[switch])\n",
    "    true_labels[switch] = torch.cat(true_labels[switch])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa973f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df85ef25-39d3-49d8-806f-d697bea2b0db",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1cbb93-2a27-483d-a8e8-a4c57721c8e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating...\n",
      "Confusion Matrix:\n",
      "[[198   2]\n",
      " [  1  99]]\n",
      "Sensitivity: 99.00%\n",
      "Specificity: 99.00%\n",
      "NPV: 99.50%\n",
      "Accuracy: 99.00%\n",
      "====================\n",
      "Confusion Matrix:\n",
      "[[376 188]\n",
      " [243 281]]\n",
      "Sensitivity: 53.63%\n",
      "Specificity: 66.67%\n",
      "NPV: 60.74%\n",
      "Accuracy: 60.39%\n",
      "====================\n",
      "Confusion Matrix:\n",
      "[[515  49]\n",
      " [271 253]]\n",
      "Sensitivity: 48.28%\n",
      "Specificity: 91.31%\n",
      "NPV: 65.52%\n",
      "Accuracy: 70.59%\n",
      "====================\n",
      "Confusion Matrix:\n",
      "[[479  85]\n",
      " [316 208]]\n",
      "Sensitivity: 39.69%\n",
      "Specificity: 84.93%\n",
      "NPV: 60.25%\n",
      "Accuracy: 63.14%\n",
      "====================\n",
      "NoSuppression-Gusarev\n",
      "T-score : 7.39947972493679\n",
      "McNemar Contingency Table:---\n",
      "[[118 351]\n",
      " [184 435]]\n",
      "McNemar Chi-Squared : 51.506542056074764\n",
      "NoSuppression-Rajaraman\n",
      "T-score : 8.105098072846442\n",
      "McNemar Contingency Table:---\n",
      "[[131 338]\n",
      " [162 457]]\n",
      "McNemar Chi-Squared : 61.25\n",
      "Gusarev-Rajaraman\n",
      "T-score : 0.4325495853408504\n",
      "McNemar Contingency Table:---\n",
      "[[ 81 221]\n",
      " [212 574]]\n",
      "McNemar Chi-Squared : 0.14780600461893764\n",
      "Complete\n",
      "                   Name  Sensitivity  Specificity       NPV  Accuracy\n",
      "0  internal_test_Daniel     0.990000     0.990000  0.994975  0.990000\n",
      "1         NoSuppression     0.536260     0.666667  0.607431  0.603860\n",
      "2               Gusarev     0.482824     0.913121  0.655216  0.705882\n",
      "3             Rajaraman     0.396947     0.849291  0.602516  0.631434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#sns.set(font_scale=1.6)\n",
    "\n",
    "def prediction_outputs(preds, true_labels=None, n_class = 2, title=\"\", save_file=None, plot_figure=False, verbose=True):\n",
    "    if true_labels is not None:\n",
    "        if preds.ndim ==2:\n",
    "            preds_closest_ints = preds.argmax(axis=1)\n",
    "        elif preds.ndim==1:\n",
    "            preds_closest_ints = preds\n",
    "        idx = preds_closest_ints == true_labels\n",
    "        \n",
    "        CM = confusion_matrix(true_labels, preds_closest_ints)\n",
    "        \n",
    "        if plot_figure:\n",
    "            fig = plt.figure(figsize=(5,5))\n",
    "            ax = plt.axes()\n",
    "            hm = sns.heatmap(CM, annot=True, \n",
    "                        fmt='d', cmap='Blues')\n",
    "            hm.set_title(title)\n",
    "            hm.set_xlabel(\"Predicted\")\n",
    "            hm.set_ylabel(\"True\")\n",
    "            hm.set_xticklabels(labels=(\"NotCOVID\",\"COVID\"), ha='center')\n",
    "            hm.set_yticklabels(labels=(\"NotCOVID\",\"COVID\"), ha='center', rotation=90)\n",
    "            #ax.set(title=title, xlabel=\"Predicted\", ylabel=\"True\")\n",
    "            if save_file is not None:\n",
    "                plt.savefig(fname=save_file)\n",
    "            plt.show()\n",
    "        \n",
    "        num_classes = []\n",
    "        if n_class == 2:\n",
    "            num_classes.append(CM[0][0] + CM[0][1])\n",
    "            num_classes.append(CM[1][0] + CM[1][1])\n",
    "            \n",
    "            sensitivity = CM[1][1]/(CM[1][1] + CM[1][0]) # TP/(TP+FN)\n",
    "            specificity = CM[0][0]/(CM[0][0] + CM[0][1]) # TN/(TN+FP)\n",
    "            NPV = CM[0][0]/(CM[0][0] + CM[1][0]) # TN/(TN+FN)\n",
    "            overacc =(CM[0][0]+CM[1][1])/ (num_classes[0]+num_classes[1])\n",
    "        if n_class == 3:\n",
    "            overacc =(CM[0][0]+CM[1][1]+CM[2][2])/ (num_classes[0]+num_classes[1]+num_classes[2])\n",
    "        if verbose:\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(CM)\n",
    "            print('Sensitivity: {sens:.2f}%'.format(sens=sensitivity*100))\n",
    "            print('Specificity: {spec:.2f}%'.format(spec=specificity*100))\n",
    "            print('NPV: {npv:.2f}%'.format(npv=NPV*100))\n",
    "            print('Accuracy: {acc:.2f}%'.format(acc=overacc*100))\n",
    "        \n",
    "        outputs={\"CM\":CM, \"Sensitivity\":sensitivity, \"Specificity\":specificity, \"NPV\":NPV, \"Accuracy\":overacc}\n",
    "        return outputs\n",
    "    else:\n",
    "        if n_class ==2:\n",
    "            idx_nonCovid = preds[:,0] > preds[:,1]\n",
    "            idx_Covid =  preds[:,0] <= preds[:,1]\n",
    "            print(\"Proportion classed as non-COVID: \" +str(np.sum(idx_nonCovid)/len(idx_nonCovid)))\n",
    "            print(\"Proportion classed as COVID: \" +str(np.sum(idx_Covid)/len(idx_Covid)))\n",
    "        if n_class ==3:\n",
    "            # 3 class\n",
    "            idx_normal = np.logical_and( preds[:,0] > preds[:,1] , preds[:,0] > preds[:,2])\n",
    "            idx_pneumonia = np.logical_and( preds[:,1] > preds[:,0] , preds[:,1] > preds[:,2])\n",
    "            idx_COVID = np.logical_and( preds[:,2] > preds[:,1] , preds[:,2] > preds[:,0])\n",
    "\n",
    "            print(\"Proportion classed as normal: \" +str(np.sum(idx_normal)/len(idx_normal)))\n",
    "            print(\"Proportion classed as pneumonia: \" +str(np.sum(idx_pneumonia)/len(idx_pneumonia)))\n",
    "            print(\"Proportion classed as COVID: \" +str(np.sum(idx_COVID)/len(idx_COVID)))\n",
    "\n",
    "def paired_t_test(array1, array2):\n",
    "    #array1 = array1.argmax(axis=1)\n",
    "    #array2 = array2.argmax(axis=1)\n",
    "    \n",
    "    x_diff = array1 - array2\n",
    "    s_diff = np.std(x_diff)\n",
    "    x_diff_mean = np.mean(x_diff)\n",
    "    s_x = s_diff/np.sqrt(len(x_diff))\n",
    "    \n",
    "    t_score = np.abs(x_diff_mean/s_x)\n",
    "    return t_score\n",
    "\n",
    "def McNemar_test(test1, test2, negative_label=0, positive_label=1):\n",
    "    #test1 = test1.argmax(axis=1)\n",
    "    #test2 = test2.argmax(axis=1)\n",
    "    \n",
    "    test1_negative = test1==negative_label\n",
    "    test1_positive = test1==positive_label\n",
    "    test2_negative = test2==negative_label\n",
    "    test2_positive = test2==positive_label\n",
    "    \n",
    "    # Contingency Table\n",
    "    a = np.logical_and(test1_positive,test2_positive)\n",
    "    b = np.logical_and(test1_positive,test2_negative)\n",
    "    c = np.logical_and(test1_negative,test2_positive)\n",
    "    d = np.logical_and(test1_negative,test2_negative)\n",
    "    \n",
    "    # McNemar Table:\n",
    "    a = np.sum(a); b = np.sum(b) ; c = np.sum(c) ; d=np.sum(d)\n",
    "    print(\"McNemar Contingency Table:---\")\n",
    "    print(np.asarray([[a,b],[c,d]]))\n",
    "    return (np.abs(b-c)-1)**2/(b+c)\n",
    "\n",
    "\n",
    "# Calculate DL-only classification\n",
    "print(\"Calculating...\")\n",
    "pandas_dict = {\"Name\":[], \"Sensitivity\":[], \"Specificity\":[], \"NPV\": [], \"Accuracy\":[]}\n",
    "for counter, status in enumerate(switch_pair):\n",
    "    a = Path(os.path.join(save_picture_directory))\n",
    "    a.mkdir(parents=True, exist_ok=True)\n",
    "    output = prediction_outputs(predictions[status].numpy(), true_labels[status].numpy() , 2, title=status, save_file=os.path.join(a,status+\".png\"))\n",
    "    #if counter > 0:\n",
    "    #    print(\"T-score {}: {}\".format(status,paired_t_test(predictions[\"Baseline\"].numpy(), predictions[status].numpy())))\n",
    "    #    print(\"McNemar Chi-Squared {}: {}\".format(status,McNemar_test(predictions[\"Baseline\"].numpy(), predictions[status].numpy())))\n",
    "    print(\"====================\")\n",
    "    # output to pandas\n",
    "    pandas_dict[\"Name\"].append(status)\n",
    "    pandas_dict[\"Sensitivity\"].append(output[\"Sensitivity\"])\n",
    "    pandas_dict[\"Specificity\"].append(output[\"Specificity\"])\n",
    "    pandas_dict[\"NPV\"].append(output[\"NPV\"])\n",
    "    pandas_dict[\"Accuracy\"].append(output[\"Accuracy\"])\n",
    "    \n",
    "\n",
    "for i, switch_i in enumerate(switch_pair):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    for switch_j in switch_pair[i+1:]:\n",
    "        print(\"{}-{}\".format(switch_i, switch_j))\n",
    "        print(\"T-score : {}\".format( paired_t_test(predictions[switch_i].numpy(), predictions[switch_j].numpy())))\n",
    "        print(\"McNemar Chi-Squared : {}\".format( McNemar_test(predictions[switch_i].numpy(), predictions[switch_j].numpy())))\n",
    "print(\"Complete\")\n",
    "df = pd.DataFrame(pandas_dict)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d84a64c-dd6f-4be3-960b-5ec28ab2b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal_test_Daniel\n",
      "AUC: 0.99\n",
      "AUC COV: 3.7437185929648245e-05\n",
      "95% AUC CI: [0.97800778 1.        ]\n",
      "NoSuppression\n",
      "AUC: 0.6014631043256997\n",
      "AUC COV: 0.00021755211896075345\n",
      "95% AUC CI: [0.57255433 0.63037188]\n",
      "Gusarev\n",
      "AUC: 0.6979724974284012\n",
      "AUC COV: 0.00015458894016292434\n",
      "95% AUC CI: [0.67360352 0.72234147]\n",
      "Rajaraman\n",
      "AUC: 0.6231186725136701\n",
      "AUC COV: 0.00017126294911816854\n",
      "95% AUC CI: [0.59746912 0.64876823]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Tue Nov  6 10:06:52 2018\n",
    "\n",
    "@author: yandexdataschool\n",
    "\n",
    "Original Code found in:\n",
    "https://github.com/yandexdataschool/roc_comparison\n",
    "\n",
    "updated: Raul Sanchez-Vazquez\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "alpha = .95\n",
    "for switch in switch_pair:\n",
    "    print(switch)\n",
    "    y_pred = predictions[switch]\n",
    "    y_true = true_labels[switch]\n",
    "\n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        y_true,\n",
    "        y_pred)\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc,\n",
    "        scale=auc_std)\n",
    "\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    print('AUC:', auc)\n",
    "    print('AUC COV:', auc_cov)\n",
    "    print('95% AUC CI:', ci)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
